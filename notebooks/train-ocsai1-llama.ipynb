{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"llama2-7b\": \"meta/llama-2-7b:77dde5d6c56598691b9008f7d123a18d98f40e4b4978f8a72215ebfc2553ddd8\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, need to create the destination model: https://replicate.com/create\n",
    "\n",
    "The hardware setting here is for inference; for training, the model will use `8x A40 (Large)` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = \"llama2-7b\"\n",
    "\n",
    "training = replicate.trainings.create(\n",
    "  version=models[model_to_train],\n",
    "  input={\n",
    "    \"train_data\": \"https://gist.githubusercontent.com/organisciak/463f2f872dac7ae39629bd94c45f208d/raw/1b592ed942880e71dfa609c16c1450c6ad8f57b5/finetune-gt_main2_prepared_train.jsonl\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"validation_data\": \"https://gist.githubusercontent.com/organisciak/463f2f872dac7ae39629bd94c45f208d/raw/1b592ed942880e71dfa609c16c1450c6ad8f57b5/finetune-gt_main2_prepared_val.jsonl\"\n",
    "  },\n",
    "  destination=\"organisciak/ocsai-llama2-70b\"\n",
    ")\n",
    "\n",
    "print(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import replicate\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model = \"organisciak/ocsai-llama2-70b:b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218\"\n",
    "\n",
    "with open('../data/ocsai1/finetune-gt_main2_prepared_test.jsonl') as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run asynchronously.\n",
    "\n",
    "This need Python 3.11+\n",
    "\n",
    "Run in batches, to save intermediate progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/303 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 201 Created\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/models/organisciak/ocsai-llama2-70b/versions/b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/jzkl7zdbxtazhxt4zpw465xrli \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/jzkl7zdbxtazhxt4zpw465xrli \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/jzkl7zdbxtazhxt4zpw465xrli \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/jzkl7zdbxtazhxt4zpw465xrli \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/jzkl7zdbxtazhxt4zpw465xrli \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/jzkl7zdbxtazhxt4zpw465xrli \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/gb6wcrlb57qmxzcxvgeetawvnq \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/ahzcx2dbet47tzfh2ti4tueboa \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/7bo4md3bhdrvtisoh4tdc56gjy \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/d2nhi6tbcax4hq3mvkiv5yfosm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/6j6pxsdbjxvjyesbii5ds6xiai \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/qtr23s3bzo4tfzqf3kuocjxd6y \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/kbmiprtb6sx3z6rrrk23odytze \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/ostgnh3b35urb63g243y5tzxbu \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.replicate.com/v1/predictions/e4uxyxtbdiicif7dt5xpenlyzm \"HTTP/1.1 200 OK\"\n",
      "  0%|          | 1/303 [00:12<1:02:37, 12.44s/it]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_data), batch_size)):\n\u001b[1;32m     16\u001b[0m     batch \u001b[38;5;241m=\u001b[39m test_data[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 17\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_batch(batch)\n\u001b[1;32m     18\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(batch_results)\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(batch):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTaskGroup() \u001b[38;5;28;01mas\u001b[39;00m tg:\n\u001b[1;32m      3\u001b[0m         tasks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/taskgroups.py:136\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, et, exc, tb)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Propagate CancelledError if there is one, except if there\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# are other errors -- those have priority.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m propagate_cancellation_error \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errors:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m propagate_cancellation_error\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m et \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m et \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errors\u001b[38;5;241m.\u001b[39mappend(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/taskgroups.py:112\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, et, exc, tb)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_completed_fut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_completed_fut\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# Our parent task is being cancelled:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# \"wrapper\" is being cancelled while \"foo\" is\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# still running.\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "async def process_batch(batch):\n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        tasks = []\n",
    "        for ex in batch:\n",
    "            inputdict = {\"prompt\": ex['prompt'] + '\\n', \"temperature\": 0.01}\n",
    "            rep_run = replicate.async_run(model, input=inputdict)\n",
    "            tasks.append(tg.create_task(rep_run))\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [\"\".join(result).strip() for result in results]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "all_results = []\n",
    "for i in tqdm(range(0, len(test_data), batch_size)):\n",
    "    batch = test_data[i:i + batch_size]\n",
    "    batch_results = await process_batch(batch)\n",
    "    all_results.extend(batch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or not async:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3030 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3030/3030 [1:07:40<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for ex in tqdm(test_data):\n",
    "    inputdict = {\"prompt\": ex['prompt'] + '\\n', \"temperature\": 0.01}\n",
    "    try:\n",
    "        result = replicate.run(model, input=inputdict)\n",
    "        result = \"\".join(result).strip()\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        logging.exception(f\"Error with {ex['prompt']}\")\n",
    "        result = \"ERROR\"\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Scoring Function\n",
    "\n",
    "Keeping here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = 'lightbulb'\n",
    "response = 'toy'\n",
    "\n",
    "prompt_template = \"AUT Prompt:{}\\nResponse:{}\\nScore:\\n\\n\"\n",
    "\n",
    "def score_llama(item, response,\n",
    "                model=\"organisciak/ocsai-llama2-70b:b00751d00cca65ff9213aea7d4fc79b9f91d2af25c5f097bd2d9fd29cc952218\"):\n",
    "    \n",
    "    output = replicate.run(\n",
    "        model,\n",
    "        input={\n",
    "            \"debug\": False,\n",
    "            \"top_p\": 1,\n",
    "            \"prompt\": prompt_template.format(item, response),\n",
    "            \"temperature\": 0.01,\n",
    "            \"return_logits\": False,\n",
    "            \"max_new_tokens\": 3,\n",
    "            \"min_new_tokens\": -1,\n",
    "            \"repetition_penalty\": 1\n",
    "        }, \n",
    "    )\n",
    "    result = \"\".join(output).strip()\n",
    "    try:\n",
    "        result = int(result) / 10\n",
    "    except:\n",
    "        print(\"Error casting to int, returning as is:\", result)\n",
    "    return result\n",
    "score_llama(item, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6906771628853364, pvalue=0.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "rows = []\n",
    "for score, ex in zip(all_results, test_data):\n",
    "    ex['predicted'] = int(score) / 10\n",
    "    #ex['target'] = int(ex['completion']) / 10\n",
    "    rows.append(ex)\n",
    "df = pd.DataFrame(rows)\n",
    "df['gptprompt'] = df.prompt\n",
    "df['prompt'] = df.gptprompt.apply(lambda x: x.split('AUT Prompt:')[1].split('\\n')[0].strip())\n",
    "df['response'] = df.gptprompt.apply(lambda x: x.split('Response:')[1].split('\\n')[0].strip())\n",
    "pearsonr(df.target, df.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3032, 19)\n",
      "Saving to ../../Data/evaluation/gt_main2/llama-ft-7b-1.csv\n"
     ]
    }
   ],
   "source": [
    "testdata = pd.read_parquet('../data/ocsai1/gt_main2_testdata.parquet').drop(columns=['predicted'])\n",
    "# clean for merging\n",
    "testdata['prompt'] = testdata['prompt'].str.strip()\n",
    "testdata['response'] = testdata['response'].str.strip()\n",
    "testdata = testdata.merge(df.drop(columns=['target']), on=['prompt', 'response'])\n",
    "print(testdata.shape)\n",
    "\n",
    "# save in same format as old data\n",
    "s = 'gt_main2'\n",
    "finetuned_size = '7b'\n",
    "finetuned_proportion = 1\n",
    "testdata['model'] = f\"llama-{finetuned_size}\"\n",
    "testdata['proportion'] = finetuned_proportion\n",
    "testdata['type'] = 'uses'\n",
    "fname  = f'llama-ft-{finetuned_size}-{finetuned_proportion}.csv'\n",
    "\n",
    "#testdata['predicted'] = testdata.predicted_raw.str.strip().str.replace('[\\-\\:/]','', regex=True).apply(lambda x:x.split(' ')[0])\n",
    "#testdata['predicted'] = pd.to_numeric(testdata['predicted'], errors='coerce').div(10)\n",
    "returncols = ['id', 'model', 'type', 'participant', 'prompt', 'target', 'predicted', 'src', 'total_tokens', 'proportion']\n",
    "output = testdata[returncols]\n",
    "base_dir = Path('../../')\n",
    "print(\"Saving to\", (base_dir / 'Data' / 'evaluation' / s / fname))\n",
    "output.to_csv(base_dir / 'Data' / 'evaluation' / s / fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
